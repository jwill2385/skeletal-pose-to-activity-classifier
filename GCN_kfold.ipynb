{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script will be used to train and run a GCN model for classifying basketball activity<br>\n",
    "Users must manually input the data parquet files where each row is a separate basketball event that tracks player joints over a 21 frame window. Note this script takes in flattened data values.<br>\n",
    "For example lets say we were analyzing a combination with 10 joints, the pose_data column contains a single flattend row of the 21 frames * 10 joints * 3 (x,y,z) position of each joint. This data needs to be unflattened before processed to easily map each coordinate to the correct joint\n",
    "\n",
    "*The input dataframe has the following columns of interest*:\n",
    "\n",
    " ['game_id', 'stadium_id', 'player_id', 'team_id',\n",
    "'event_seq_id', 'event_type', 'ball_position', 'player_com', 'pose_data', 'label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing and Graph Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader as GeometricDataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn as nn\n",
    "from torch.nn import GRU\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Define Global Parameters\n",
    "\n",
    "NUM_JOINTS = 16 #  NOTE: Adjust if your data uses a different number of joints\n",
    "\n",
    "\n",
    "# Explicitly define the joint columns and limb pairs for the GCN\n",
    "joint_columns = ['rShoulder', 'lShoulder', 'rElbow', 'lElbow', 'rWrist', 'lWrist', 'neck', 'rKnee', 'lKnee', 'rAnkle', 'lAnkle', 'rHip', 'lHip', 'midHip', 'rHeel', 'lHeel'] \n",
    "# Unlike the transformer, for the GCN we have to also define the graph struture where limb pairs define our edges\n",
    "\n",
    "limb_pairs = [  \n",
    "    ('lElbow', 'lWrist'),\n",
    "    ('rElbow', 'rWrist'),\n",
    "    ('lShoulder', 'lElbow'),\n",
    "    ('rShoulder', 'rElbow'),\n",
    "    ('lShoulder', 'neck'),\n",
    "    ('rShoulder', 'neck'),\n",
    "    ('neck', 'midHip'),\n",
    "    ('midHip', 'lHip'),\n",
    "    ('midHip', 'rHip'),\n",
    "    ('lShoulder', 'lHip'),\n",
    "    ('rShoulder', 'rHip'),\n",
    "    ('lHip', 'lKnee'), \n",
    "    ('rHip', 'rKnee'),\n",
    "    ('lKnee', 'lAnkle'),\n",
    "    ('rKnee', 'rAnkle'),\n",
    "    ('lAnkle', 'lHeel'),\n",
    "    ('rAnkle', 'rHeel')\n",
    "] \n",
    " \n",
    "\n",
    "\n",
    "# Create Graph Edge Index\n",
    "\n",
    "def create_edge_index():\n",
    "    edge_index = []\n",
    "    joint_to_index = {joint: i for i, joint in enumerate(joint_columns)}\n",
    "    for limb in limb_pairs:\n",
    "        edge_index.append((joint_to_index[limb[0]], joint_to_index[limb[1]]))\n",
    "        edge_index.append((joint_to_index[limb[1]], joint_to_index[limb[0]]))  # both directions\n",
    "    return torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "EDGE_INDEX = create_edge_index()\n",
    "\n",
    "\n",
    "# Load Data\n",
    "\n",
    "def load_data(dribble_file, pass_file, shot_file, rebound_file):\n",
    "    \"\"\"\n",
    "    Loads parquet files, concatenates, filters out invalid pose data,\n",
    "    and maps string event_type to numeric labels: (0=dribble, 1=pass, 2=shot, 3=rebound).\n",
    "    \"\"\"\n",
    "    dribble_df = pd.read_parquet(dribble_file)\n",
    "    pass_df    = pd.read_parquet(pass_file)\n",
    "    shot_df    = pd.read_parquet(shot_file)\n",
    "    rebound_df = pd.read_parquet(rebound_file)\n",
    "\n",
    "    all_data = pd.concat([dribble_df, pass_df, shot_df, rebound_df], ignore_index=True)\n",
    "\n",
    "    # Filter out rows where pose_data is not the right length\n",
    "    expected_pose_len = 21 * NUM_JOINTS * 3\n",
    "    all_data = all_data[\n",
    "        all_data['pose_data'].apply(\n",
    "            lambda x: len(x) == expected_pose_len if isinstance(x, (list, np.ndarray)) else False\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Map event_type to label\n",
    "    event_mapping = {'dribble': 0, 'pass': 1, 'shot': 2, 'rebound': 3}\n",
    "    all_data['label'] = all_data['event_type'].map(event_mapping)\n",
    "\n",
    "    return all_data\n",
    "\n",
    "# Split Dataframe into Train, Val, Test\n",
    "\n",
    "def split_dataframe(all_data):\n",
    "    \"\"\"\n",
    "    Splits the data into train (60%), val (20%), test (20%) with stratification by 'label'.\n",
    "    \"\"\"\n",
    "    train_df, temp_df = train_test_split(\n",
    "        all_data, test_size=0.4, random_state=42, stratify=all_data['label']\n",
    "    )\n",
    "    val_df, test_df = train_test_split(\n",
    "        temp_df, test_size=0.5, random_state=42, stratify=temp_df['label']\n",
    "    )\n",
    "    print(f\"Train size: {len(train_df)}, Val size: {len(val_df)}, Test size: {len(test_df)}\")\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "\n",
    "# Balance Training Data\n",
    "# Downsample passes if bigger than dribbles,\n",
    "# Upsample shots & rebounds if smaller than dribbles.\n",
    "\n",
    "def balance_train_df(train_df, noise_for_upsampled=0.02):\n",
    "    \"\"\"\n",
    "    - Dribble -> label=0\n",
    "    - Pass -> label=1\n",
    "    - Shot -> label=2\n",
    "    - Rebound -> label=3\n",
    "    \n",
    "    We'll match the size of the dribble class. \n",
    "    \"\"\"\n",
    "    dribble_df = train_df[train_df['label'] == 0]\n",
    "    pass_df    = train_df[train_df['label'] == 1]\n",
    "    shot_df    = train_df[train_df['label'] == 2]\n",
    "    rebound_df = train_df[train_df['label'] == 3]\n",
    "\n",
    "    # Target size is the dribble class size\n",
    "    target_size = len(dribble_df)\n",
    "    print(f\"Dribble size => {target_size}\")\n",
    "\n",
    "    # Downsample Pass if pass_df is larger than dribble_df\n",
    "    if len(pass_df) > target_size:\n",
    "        pass_df = pass_df.sample(n=target_size, replace=False, random_state=42)\n",
    "        print(f\"Downsampled pass to {len(pass_df)}\")\n",
    "    else:\n",
    "        print(f\"Pass count => {len(pass_df)} (no downsampling needed)\")\n",
    "\n",
    "    # Upsample Shots if smaller\n",
    "    if len(shot_df) < target_size:\n",
    "        diff = target_size - len(shot_df)\n",
    "        # randomly select indicies to upsample\n",
    "        upsample_indices = np.random.choice(shot_df.index, size=diff, replace=True)\n",
    "        shot_upsampled = shot_df.loc[upsample_indices]\n",
    "        # add random jitter to the new upsampled joint positions\n",
    "        shot_upsampled_aug = shot_upsampled.apply(\n",
    "            lambda row: noise_augment_pose(row, noise_level=noise_for_upsampled), axis=1\n",
    "        )\n",
    "        shot_df = pd.concat([shot_df, shot_upsampled_aug], ignore_index=True)\n",
    "        print(f\"Upsampled shots to {len(shot_df)}\")\n",
    "\n",
    "    # Upsample Rebounds if smaller\n",
    "    if len(rebound_df) < target_size:\n",
    "        diff = target_size - len(rebound_df)\n",
    "        upsample_indices = np.random.choice(rebound_df.index, size=diff, replace=True)\n",
    "        rebound_upsampled = rebound_df.loc[upsample_indices]\n",
    "        # add random jitter to the new upsampled joint positions\n",
    "        rebound_upsampled_aug = rebound_upsampled.apply(\n",
    "            lambda row: noise_augment_pose(row, noise_level=noise_for_upsampled), axis=1\n",
    "        )\n",
    "        rebound_df = pd.concat([rebound_df, rebound_upsampled_aug], ignore_index=True)\n",
    "        print(f\"Upsampled rebounds to {len(rebound_df)}\")\n",
    "\n",
    "    balanced_train = pd.concat([dribble_df, pass_df, shot_df, rebound_df], ignore_index=True)\n",
    "    print(f\"Balanced train size => {len(balanced_train)}\")\n",
    "    return balanced_train\n",
    "\n",
    "\n",
    "# Data Augmentation Helper\n",
    "# Added random noise to pose data when upsampling\n",
    "def noise_augment_pose(row, noise_level=0.02):\n",
    "    \"\"\"\n",
    "    row: a single row containing row['pose_data'] with shape [21*NUM_JOINTS*3].\n",
    "    Returns a copy of the row with added Gaussian noise on 'pose_data'.\n",
    "    \"\"\"\n",
    "    pose_arr = np.array(row['pose_data']).reshape(21, NUM_JOINTS, 3)\n",
    "    noise = np.random.normal(loc=0, scale=noise_level, size=pose_arr.shape)\n",
    "    augmented = pose_arr + noise\n",
    "\n",
    "    row_copy = row.copy()\n",
    "    row_copy['pose_data'] = augmented.flatten()\n",
    "    return row_copy\n",
    "\n",
    "\n",
    "# Convert Dataframe Rows to PyTorch Geometric Data\n",
    "# This defines the graph structure for the GCN\n",
    "\n",
    "def convert_df_to_pyg_dataset(df):\n",
    "    data_list = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Reshape -> [21, NUM_JOINTS, 3]\n",
    "        pose_data = np.array(row['pose_data']).reshape(21, NUM_JOINTS, 3)\n",
    "        velocity = np.diff(pose_data, axis=0, prepend=pose_data[0:1])\n",
    "        acceleration = np.diff(velocity, axis=0, prepend=velocity[0:1])\n",
    "        # Reshape -> [21, NUM_JOINTS, 9] if using pose + velocity + acceleration\n",
    "        combined = np.concatenate([pose_data, velocity, acceleration], axis=-1)\n",
    "        node_feats = torch.tensor(combined.reshape(-1, 9), dtype=torch.float)\n",
    "        # NOTE:To just use pose_data uncomment below:\n",
    "        # node_feats = torch.tensor(pose_data.reshape(-1, 3), dtype=torch.float)\n",
    "\n",
    "        label = torch.tensor(row['label'], dtype=torch.long)\n",
    "        graph = Data(x=node_feats, edge_index=EDGE_INDEX, y=label)\n",
    "        data_list.append(graph)\n",
    "    return data_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCN Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define GCN Model (EnhancedActionClassifierGCN)\n",
    "class EnhancedActionClassifierGCN(nn.Module):\n",
    "    def __init__(self, node_feature_dim, hidden_dim, num_classes, dropout):\n",
    "        super(EnhancedActionClassifierGCN, self).__init__()\n",
    "        self.conv1 = GCNConv(node_feature_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv4 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.gru = GRU(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc2 = nn.Linear(hidden_dim // 2, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        # Graph convolutions\n",
    "        x = torch.relu(self.conv1(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.conv2(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.conv3(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.conv4(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Reshape for GRU\n",
    "        batch_size = data.batch.max().item() + 1\n",
    "        seq_length = x.size(0) // batch_size // NUM_JOINTS\n",
    "        x = x.view(batch_size, seq_length, NUM_JOINTS, -1)  # [B, seq_len, num_joints, hidden_dim]\n",
    "        x = x.mean(dim=2)  # average across joints -> [B, seq_len, hidden_dim]\n",
    "\n",
    "        # GRU\n",
    "        x, _ = self.gru(x)       # [B, seq_len, hidden_dim]\n",
    "        x = x[:, -1, :]          # take last timestep -> [B, hidden_dim]\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train and Evaluate the GCN Model\n",
    "\n",
    "def train_epoch(loader, model, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch)\n",
    "        loss = criterion(out, batch.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(loader, model, device):\n",
    "    model.eval()\n",
    "    num_classes = 4\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    class_correct = [0] * num_classes\n",
    "    class_total = [0] * num_classes\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            out = model(batch)\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct += (pred == batch.y).sum().item()\n",
    "            total += batch.y.size(0)\n",
    "\n",
    "            # Per-Class Accuracy Tracking\n",
    "            for label, prediction in zip(batch.y, pred):\n",
    "                class_total[label] += 1\n",
    "                if label == prediction:\n",
    "                    class_correct[label] += 1\n",
    "    overall_acc = 100 * correct / total \n",
    "    class_accuracies = [\n",
    "        100 * class_correct[i] / class_total[i] if class_total[i] > 0 else 0.0\n",
    "        for i in range(num_classes)\n",
    "    ]\n",
    "\n",
    "    return overall_acc, class_accuracies\n",
    "\n",
    "\n",
    "# Generate Confusion Matrix to evaluate model performance\n",
    "\n",
    "def generate_confusion_matrix(model, test_data, device):\n",
    "    test_loader = GeometricDataLoader(test_data, batch_size=32, shuffle=False)\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            out =+ model(batch)\n",
    "            preds = out.argmax(dim=1).cpu().numpy()\n",
    "            labels = batch.y.cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=[0, 1, 2, 3])\n",
    "    # Normalize the confusion matrix by dividing each row by the sum of its elements\n",
    "    # This is to display percentages\n",
    "    conf_matrix_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Calculate overall accuracy\n",
    "    total_correct = np.trace(cm)\n",
    "    total = np.sum(cm)\n",
    "    overall_accuracy = total_correct / total\n",
    "    print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n",
    "\n",
    "    # Calculate per class accuracy\n",
    "    # The diagonal of the normalized matrix contains the per-class accuracy\n",
    "    per_class_acc = np.diagonal(conf_matrix_normalized)\n",
    "    class_names = [\"dribble\",\"pass\",\"shot\",\"rebound\"]\n",
    "    print(\"\\n Per-Class Accuracy (from normalized diagonal):\")\n",
    "    for i, acc in enumerate(per_class_acc):\n",
    "        print(f\"  {class_names[i]}: {acc:.4f}\")\n",
    "    \n",
    "    # Display the confusion matrix\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(f\"Confusion Matrix J{NUM_JOINTS}\")\n",
    "    plt.show()\n",
    "\n",
    "    # Display the percentage confusion matrix\n",
    "    disp_percentage = ConfusionMatrixDisplay(confusion_matrix=conf_matrix_normalized, display_labels=class_names)\n",
    "    disp_percentage.plot(cmap='Blues', values_format='.02%')  # Format to show percentages with 2 decimal places\n",
    "    plt.title(f\"Confusion Matrix (Percentages) J{NUM_JOINTS}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Stratified Kfold processing\n",
    "def regular_k_fold_evaluation(dataframe, k=5, best_params=None):\n",
    "    from torch_geometric.loader import DataLoader as GeometricDataLoader\n",
    "    from sklearn.model_selection import KFold\n",
    "\n",
    "    # Setup GPU for training\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    num_classes = 4\n",
    "    # define model parameters\n",
    "    hidden_dim   = best_params[\"hidden_dim\"]\n",
    "    dropout      = best_params[\"dropout\"]\n",
    "    learning_rate= best_params[\"lr\"]\n",
    "    batch_size   = best_params[\"batch_size\"]\n",
    "\n",
    "    # need to generate labels for stratified k fold\n",
    "    labels = dataframe['label'].values\n",
    "\n",
    "    #lets use stratified k fold to maintain class distribution\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    fold_accuracies = []\n",
    "    fold_class_accuracies = {i: [] for i in range(num_classes)}\n",
    "    best_model_state = None\n",
    "    best_fold_accuracy = 0\n",
    "    all_training_losses = []   # track training loss per epoch for each fold\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(dataframe, labels)):\n",
    "        print(f\"\\nFold {fold_idx + 1}/{k}\")\n",
    "\n",
    "        # Pre-Augmentation sample sizes and class mix\n",
    "        y_tr = labels[train_idx]\n",
    "        y_va = labels[val_idx]\n",
    "        tr_counts = np.bincount(y_tr, minlength=num_classes)\n",
    "        va_counts = np.bincount(y_va, minlength=num_classes)\n",
    "        \n",
    "        class_names = ['Dribble', 'Pass', 'Shot', 'Rebound']\n",
    "        print(f\"[Pre-aug] train_n={len(train_idx)}  val_n={len(val_idx)}\")\n",
    "        print(\"train per-class:\", {cls: int(tr_counts[i]) for i, cls in enumerate(class_names)})\n",
    "        print(\"val   per-class:\", {cls: int(va_counts[i]) for i, cls in enumerate(class_names)})\n",
    "\n",
    "\n",
    "        train_df = dataframe.iloc[train_idx].reset_index(drop=True)\n",
    "        val_df = dataframe.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "        balanced_train_df = balance_train_df(train_df)\n",
    "        train_data = convert_df_to_pyg_dataset(balanced_train_df)\n",
    "        val_data = convert_df_to_pyg_dataset(val_df)\n",
    "\n",
    "        train_loader = GeometricDataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = GeometricDataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # If using only pose data then node_feature_dim = 3\n",
    "        # If using pose + velocity + acceleration then node_feature_dim = 9\n",
    "        model = EnhancedActionClassifierGCN(\n",
    "            node_feature_dim=9,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_classes=4,\n",
    "            dropout=dropout\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Training loop\n",
    "        training_losses = []\n",
    "        for epoch in range(10):\n",
    "            train_loss = train_epoch(train_loader, model, optimizer, criterion, device)\n",
    "            training_losses.append(train_loss)\n",
    "            print(f\"Epoch {epoch+1}/10 - Loss: {train_loss:.4f}\")\n",
    "\n",
    "        fold_accuracy, class_accuracies = evaluate(val_loader, model, device)\n",
    "        fold_accuracies.append(fold_accuracy)\n",
    "        all_training_losses.append(training_losses)\n",
    "\n",
    "        if fold_accuracy > best_fold_accuracy:\n",
    "            best_fold_accuracy = fold_accuracy\n",
    "            best_model_state = model.state_dict()\n",
    "            # Save the best model\n",
    "            torch.save(best_model_state, f\"temporal_gcn_kfold_best_model_J{NUM_JOINTS}.pth\")\n",
    "            print(\"Best model saved after K-Fold.\")\n",
    "\n",
    "        for class_idx in range(num_classes):\n",
    "            fold_class_accuracies[class_idx].append(class_accuracies[class_idx])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Performance Metrics\n",
    "    mean_accuracy = np.mean(fold_accuracies)\n",
    "    std_accuracy = np.std(fold_accuracies)\n",
    "    ci_range = 1.96 * (std_accuracy / np.sqrt(k)) #95% CI\n",
    "\n",
    "    print(f\"\\nMean Accuracy: {mean_accuracy:.2f}% ± {ci_range:.2f}%\")\n",
    "    print(f\"Accuracy Variance: {std_accuracy:.2f}\")\n",
    "\n",
    "    # Per-Class Accuracy\n",
    "    class_names = ['Dribble', 'Pass', 'Shot', 'Rebound']\n",
    "    for class_idx in range(num_classes):\n",
    "        class_mean = np.mean(fold_class_accuracies[class_idx])\n",
    "        class_std = np.std(fold_class_accuracies[class_idx])\n",
    "        class_ci = 1.96 * class_std / np.sqrt(k)\n",
    "        print(f\"Class {class_names[class_idx]} Accuracy: {class_mean:.2f}% ± {class_ci:.2f}%\")\n",
    "\n",
    "\n",
    "    # Per-Class Accuracy Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for class_idx in range(num_classes):\n",
    "        class_acc = fold_class_accuracies[class_idx]\n",
    "        class_ci = 1.96 * (np.std(class_acc) / np.sqrt(k))\n",
    "\n",
    "        plt.errorbar(\n",
    "            x=range(1, k + 1),\n",
    "            y=class_acc,\n",
    "            yerr=class_ci,\n",
    "            fmt='o-', capsize=5, label=f'Class {class_names[class_idx]} ± CI'\n",
    "        )\n",
    "\n",
    "    plt.title(f\"Per-Class Accuracy with Confidence Intervals J{NUM_JOINTS}\")\n",
    "    plt.xlabel(\"Fold\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.grid(False)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Training Loss per Epoch for Each Fold\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, losses in enumerate(all_training_losses):\n",
    "        plt.plot(range(1, 10 + 1), losses, label=f'Fold {i + 1}')\n",
    "    plt.title(f\"Training Loss per Epoch J{NUM_JOINTS}\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "    # Accuracy Distribution (Box Plot)\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.boxplot(fold_accuracies)\n",
    "    plt.title(f\"Accuracy Distribution Across Folds J{NUM_JOINTS}\")\n",
    "    plt.xlabel(\"K-Fold Results\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    # Visualizing Confidence Intervals\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.errorbar(\n",
    "        x=range(1, k + 1),\n",
    "        y=fold_accuracies,\n",
    "        yerr=ci_range,\n",
    "        fmt='o', capsize=5, label='Fold Accuracy ± CI'\n",
    "    )\n",
    "    plt.title(f\"K-Fold Accuracy with Confidence Intervals J{NUM_JOINTS}\")\n",
    "    plt.xlabel(\"Fold\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.grid(False)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Lets return the best model state\n",
    "    model.load_state_dict(best_model_state) # it may not be the last fold which is why we have to load\n",
    "    return model\n",
    "\n",
    "def train_best_model(best_params):\n",
    "    \"\"\"\n",
    "    Retrain a final model with the best params.\n",
    "    \"\"\"\n",
    "    hidden_dim   = best_params[\"hidden_dim\"]\n",
    "    dropout      = best_params[\"dropout\"]\n",
    "    learning_rate= best_params[\"lr\"]\n",
    "    batch_size   = best_params[\"batch_size\"]\n",
    "\n",
    "    model = EnhancedActionClassifierGCN(\n",
    "        node_feature_dim=9,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_classes=4,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_loader = GeometricDataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    epochs_for_final = 10\n",
    "    for epoch in range(epochs_for_final):\n",
    "        train_loss = train_epoch(train_loader, model, optimizer, criterion, device)\n",
    "        print(f\"Epoch {epoch+1}/{epochs_for_final} - Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Main Function to Process and Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Load your data from parquet files of joint data for each event type\n",
    "    dribble_file = \" \"\n",
    "    pass_file    = \" \"\n",
    "    shot_file    = \" \"\n",
    "    rebound_file = \" \"\n",
    "\n",
    "    all_data = load_data(dribble_file, pass_file, shot_file, rebound_file)\n",
    "    print(f\"All data size (filtered) => {len(all_data)}\")\n",
    "\n",
    "\n",
    "    # Setup GPU for training\n",
    "    global device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Define best parameters. NOTE We chose best hyperparameters after running optuna and tuning for each joint combination\n",
    "    best_params = {\n",
    "        \"hidden_dim\": 256,\n",
    "        \"dropout\": 0.1017837074458662,\n",
    "        \"lr\": 0.00022862438008880502,\n",
    "        \"batch_size\": 64\n",
    "    }\n",
    "\n",
    "    # KFold evaluation to get best model. We are running this on the entire dataset\n",
    "    best_model = regular_k_fold_evaluation(all_data, k=5, best_params=best_params)\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
